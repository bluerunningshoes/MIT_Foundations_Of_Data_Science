{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ckKZFtumStuf"
   },
   "source": [
    "## Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cOdI3xgjStut"
   },
   "source": [
    "The data scientists at BigMart have collected 2013 sales data for 1559 products across 10 stores in different cities. Also, certain attributes of each product and store have been defined. The aim is to build a predictive model and find out the sales of each product at a particular store. Using this model, BigMart will try to understand the properties of products and stores which play a key role in increasing sales. Please note that the data may have missing values as some stores might not report all the data due to technical glitches. Hence, it will be required to treat them accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BucVDDLTStuw"
   },
   "source": [
    "## Objective\n",
    "\n",
    "To build a predictive model and find out the sales of each product at a particular store. And then provide recommendations to the BigMart sales team to understand the properties of products and stores which play a key role in increasing sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "di8-QKv8Stu0"
   },
   "source": [
    "## Data Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MdwekZv2Stu2"
   },
   "source": [
    "- **Item_Identifier** : Unique product ID\n",
    "\n",
    "- **Item_Weight** : Weight of product\n",
    "\n",
    "- **Item_Fat_Content** : Whether the product is low fat or not\n",
    "\n",
    "- **Item_Visibility** : The % of the total display area of all products in a store allocated to the particular product\n",
    "\n",
    "- **Item_Type** : The category to which the product belongs\n",
    "\n",
    "- **Item_MRP** : Maximum Retail Price (list price) of the product\n",
    "\n",
    "- **Outlet_Identifier** : Unique store ID\n",
    "\n",
    "- **Outlet_Establishment_Year** : The year in which the store was established\n",
    "\n",
    "- **Outlet_Size** : The size of the store in terms of ground area covered\n",
    "\n",
    "- **Outlet_Location_Type** : The type of city in which the store is located\n",
    "\n",
    "- **Outlet_Type** : Whether the outlet is just a grocery store or some sort of supermarket\n",
    "\n",
    "- **Item_Outlet_Sales** : Sales of the product in the particular store. This is the outcome variable to be predicted.\n",
    "\n",
    "We have two datasets - train (8523) and test (5681) data. The train dataset has both input and output variable(s). You need to predict the sales for the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZGh2xQ6uStu5"
   },
   "source": [
    "## Loading libraries and overview of datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Xa1S_t2CStu8"
   },
   "outputs": [],
   "source": [
    "# importing libraries for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# importing libraries for data visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# importing libraries for building linear regression model\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# importing libraries for scaling the data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#to ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gIF5qYELStu_"
   },
   "source": [
    "#### Loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "yPhwd9eYStvC"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-8fc43aca82a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# loading both train and test datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             )\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \"\"\"\n\u001b[0;32m-> 1362\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Train.csv'"
     ]
    }
   ],
   "source": [
    "# loading both train and test datasets\n",
    "train_df = pd.read_csv('Train.csv')\n",
    "test_df = pd.read_csv('Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Crh3kItoStvF",
    "outputId": "e3c6c284-a72d-4ce6-e2ae-9aba4a45d3ab"
   },
   "outputs": [],
   "source": [
    "# checking the dataset\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z_aocqTOStvM"
   },
   "source": [
    "#### Observations:\n",
    "\n",
    "- As the variables `Item_Identifier` and `Outlet_Identifier` are only ID variables, we assume that they don't have any predictive power to predict the dependent variable / outcome variable - `Item_Outlet_Sales`.\n",
    "- We remove these two variables from both the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HqXfuRtnStvO"
   },
   "outputs": [],
   "source": [
    "train_df = train_df.drop(['Item_Identifier', 'Outlet_Identifier'], axis=1)\n",
    "test_df = test_df.drop(['Item_Identifier', 'Outlet_Identifier'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N_mGLhk1StvQ"
   },
   "source": [
    "Now, let's find out some more information about the dataset i.e. total number of observations in the dataset, columns and their data types, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yMQfbgkuStvS",
    "outputId": "f78fafa4-e9f4-4e9b-a188-a347a2611888"
   },
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XtIlhemsStvU"
   },
   "source": [
    "#### Observations:\n",
    "\n",
    "- The train datasets has `8523` number of observations and `10` columns. \n",
    "- Two columns- `Item_Weight` and `Outlet_Size` has missing values as their non-null values is less the total number of observations in the dataset. \n",
    "- We can also see that the columns - `Item_Fat_Content`, `Item_Type`, `Outlet_Size`, `Outlet_Location_Type` and `Outlet_Type` has data type `object` that means they are strings which means they are categorical variables. \n",
    "- Remaining all other variables are numerical in nature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K1URNcD1StvX"
   },
   "source": [
    "As we have already seen that two columns have missing values in the dataset. Let's check the percentage of missing values using the below piece of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-HvFZkt8StvZ",
    "outputId": "741c596d-582e-4a97-e018-17e656f41c4b"
   },
   "outputs": [],
   "source": [
    "(train_df.isnull().sum()/train_df.shape[0])*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rupo2xfeStvb"
   },
   "source": [
    "Now we get the percentage of missing values for the columns - `Item_Weight` and `Outlet_Size` which are ~17% and ~28% respectively. We will see next how to treat these missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nOlU9ZqbStvd"
   },
   "source": [
    "## EDA and Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OHDVC-7iStve"
   },
   "source": [
    "Now that we have an understanding of the business problem which we want to solve and also we have loaded the datasets. The next step to follow is to have a better understanding of the dataset i.e. what is the distribution of the variables, what are different relationships that exist between variables, etc. If there is any data anomaly like missing values, outliers, how do we treat them to prepare the dataset for building the predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xutlt_PpStvh"
   },
   "source": [
    "### Univariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PIALSmqwStvj"
   },
   "source": [
    "Let us now start exploring the dataset, by performing univariate analysis of the dataset i.e. analyzing/visualizing the dataset by taking one variable at a time. Data visualization is a very important skill to have and we need to decide what charts to plot to better understand the data. For example - what chart makes sense to analyze categorical variable or what charts are suited for numerical variable and also it depends on what relationship between variables we want to show?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jRI8vbAyStvm"
   },
   "source": [
    "Let's start with analyzing the `categorical` variables present in the data. There are five categorical variables in this dataset and we are creating univariate bar charts for each of them to check their distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AC7oJpD8Stvo"
   },
   "source": [
    "Below we are creating 4 subplots to plot them in a single frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zPL_2JzPStvq",
    "outputId": "49ce68f8-b7ff-4a07-8a85-e931d01c0a0d"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(18, 10))\n",
    "  \n",
    "fig.suptitle('Bar plot for all categorical variables in the dataset')\n",
    "  \n",
    "\n",
    "sns.countplot(ax=axes[0, 0], x='Item_Fat_Content', data=train_df, color='blue', \n",
    "              order=train_df['Item_Fat_Content'].value_counts().index, );\n",
    "\n",
    "sns.countplot(ax=axes[0, 1], x='Outlet_Size', data=train_df, color='blue', \n",
    "              order=train_df['Outlet_Size'].value_counts().index);\n",
    "\n",
    "sns.countplot(ax=axes[1, 0], x='Outlet_Location_Type', data=train_df, color='blue', \n",
    "              order=train_df['Outlet_Location_Type'].value_counts().index);\n",
    "\n",
    "sns.countplot(ax=axes[1, 1], x='Outlet_Type', data=train_df, color='blue', \n",
    "              order=train_df['Outlet_Type'].value_counts().index);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3CMinhGlStvs"
   },
   "source": [
    "#### Observations:\n",
    "\n",
    "- From the above univariate plot for the variable `Item_Fat_Content`, it seems like there are errors in the data. The category - `Low Fat` is sometimes also written as `low fat` and `LF`. And also the category `Regular` is also written as `reg` sometimes. So **we need to fix this issue in the data**.\n",
    "- For the column `Outlet_Size`, we can see that the most common category is - `Medium` followed by Small and High\n",
    "- The most common category for the column `Outlet_Location_Type` is `Tier 3`, followed by Tier 2 and Tier 1. This makes sense now if we combine this information with the information on column `Outlet_Size`. We would expect High outlet size stores to be present in Tier 1 cities and the count of tier 1 cities is less so the count of high outlets size is also less. And we would expect more number of medium and small number of outlet sizes in the dataset because we have more number outlets present in tier 3 and tier 2 cities in the dataset.\n",
    "- In the column `Outlet_Type`, the majority of the stores or the model is of `Supermarket Type 1` and we have less and almost equal number of representation in the other categories - Supermarket Type 2 and Supermarket Type 3 and Grocery Store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nIFZjWXtStvu"
   },
   "source": [
    "Below we are analyzing the categorical variable `Item_Type`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nt9S2yL3Stvv",
    "outputId": "9a6d6983-00a5-4a2d-ed0d-7f2c1a30f737"
   },
   "outputs": [],
   "source": [
    "fig= plt.figure(figsize=(18, 6))\n",
    "sns.countplot(x='Item_Type', data=train_df, color='blue', order=train_df['Item_Type'].value_counts().index);\n",
    "plt.xticks(rotation=45);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4pPMAibqStvx"
   },
   "source": [
    "From the above plot, we can see that majority of the the items sold in these stores are `Fruits and Vegetables`, followed by `Snack Foods`, `Household items`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ApCBE0coStvy"
   },
   "source": [
    "Before we move ahead with the univariate analysis for the numerical variables, let's first fix the data issues that we have found out for the column `Item_Fat_Content`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JWACZl35Stvz"
   },
   "source": [
    "In the below piece of code, we are replacing the categories - `low fat` and `LF` by `Low Fat` using lambda function and also we are replacing the category `reg` by Regular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x4df1we6Stv3"
   },
   "outputs": [],
   "source": [
    "train_df['Item_Fat_Content'] = train_df['Item_Fat_Content'].apply(lambda x: 'Low Fat' if x=='low fat' or x=='LF' else x)\n",
    "train_df['Item_Fat_Content'] = train_df['Item_Fat_Content'].apply(lambda x: 'Regular' if x=='reg' else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GzstIj9mStv5"
   },
   "source": [
    "The data preparation steps that we do on the training data, we need to perform the same steps on test data as well. So below we are performing the same transformation on `test` dataset - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1gwGQ5J9Stv6"
   },
   "outputs": [],
   "source": [
    "test_df['Item_Fat_Content'] = test_df['Item_Fat_Content'].apply(lambda x: 'Low Fat' if x=='low fat' or x=='LF' else x)\n",
    "test_df['Item_Fat_Content'] = test_df['Item_Fat_Content'].apply(lambda x: 'Regular' if x=='reg' else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IiEW9fw1Stv8"
   },
   "source": [
    "Below we are analyzing all the `numerical` variables present in the data. And since we want to visualize one numerical variable at a time, histogram is the best choice to visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DQ4IHUL8Stv9",
    "outputId": "8d90ac73-84da-4414-bbac-58d3a061f73c"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "  \n",
    "fig.suptitle('Histogram for all numerical variables in the dataset')\n",
    "  \n",
    "sns.histplot(x='Item_Weight', data=train_df, kde=True, ax=axes[0]);\n",
    "sns.histplot(x='Item_Visibility', data=train_df, kde=True, ax=axes[1]);\n",
    "sns.histplot(x='Item_MRP', data=train_df, kde=True, ax=axes[2]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BUH_f842StwB"
   },
   "source": [
    "#### Observations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CB1m7GczStwC"
   },
   "source": [
    "- The variable `Item_Weight` is approx uniformly distributed and when we will impute the missing values for this column we will need to keep this in mind that we don't end up changing the distribution significantly after imputing those missing values\n",
    "- The variable `Item_Visibility` is a right skewed distribution which means that there are certain items whose percentage of display area is much higher than the other items\n",
    "- The variable `Item_MRP` is following approx multi-modal normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TL6S28nYStwD"
   },
   "source": [
    "### Bivariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EHmf3_bNStwE"
   },
   "source": [
    "Now let's move ahead with bivariate analysis and to understand how variables are related to each other and if there is a strong relationship between dependent and independent variables present in the training dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F4s-kmcDStwF"
   },
   "source": [
    "In the below plot - we are analyzing the variables `Outlet_Establishment_Year` and `Item_Outlet_Sales`. Here, since the variable `Outlet_Establishment_Year` is defining a time component, the best chart to analyze this relationship will be a line plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_iJCicN8StwH",
    "outputId": "ab00f7fc-72d9-40e5-f171-0f095324744c"
   },
   "outputs": [],
   "source": [
    "fig= plt.figure(figsize=(18, 6))\n",
    "sns.lineplot(x='Outlet_Establishment_Year', y='Item_Outlet_Sales', data=train_df, ci=None, estimator='mean');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fwU75J9hStwJ"
   },
   "source": [
    "#### Observations:\n",
    "\n",
    "- The average sales are almost constant every year, but we don't see any increasing/decreasing trend in sales with time. So the variable year might not be a good predictor to predict sales, which we can see later in the modeling phase.\n",
    "- Also, in the year 1998 the average sales has plummeted. This might be due to some external factors which are not included in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z3fga3jTStwL"
   },
   "source": [
    "Next, we are trying to find out linear correlations between the variables, This will help us to know which numerical variables are correlated with the dependent/target variable. Also we can find out multi-collinearity i.e. which pair of independent variables are correlated with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PRfBJcXhStwM",
    "outputId": "a5afe68f-425e-452e-c6a9-7da120f1cfb1"
   },
   "outputs": [],
   "source": [
    "fig= plt.figure(figsize=(18, 6))\n",
    "sns.heatmap(train_df.corr(), annot=True);\n",
    "plt.xticks(rotation=45);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MwA1Fp29StwO"
   },
   "source": [
    "#### Observations:\n",
    "\n",
    "- From the above plot, it seems only the independent variable `Item_MRP` has moderate linear relationship with the dependent variable `Item_Outlet_Sales`\n",
    "- For the remaining, it does not seem like there is any strong positive/negative correlation between the variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7imH3W36StwP"
   },
   "source": [
    "Next, we are creating the bivariate scatter plots to check relationships between pairs of independent and dependent variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nhCOa7qCStwQ",
    "outputId": "947a8c19-0f17-412a-99ba-b2e01237f506"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "  \n",
    "fig.suptitle('Bi-variate scatterplot for all numerical variables with the dependent variable')\n",
    "  \n",
    "sns.scatterplot(x='Item_Weight', y='Item_Outlet_Sales', data=train_df, ax=axes[0]);\n",
    "sns.scatterplot(x='Item_Visibility', y='Item_Outlet_Sales', data=train_df, ax=axes[1]);\n",
    "sns.scatterplot(x='Item_MRP', y='Item_Outlet_Sales', data=train_df, ax=axes[2]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxo88e_CStwZ"
   },
   "source": [
    "#### Observations:\n",
    "\n",
    "- The first scatter plot shows the data is completely random and there is no relationship between `Item_Weight` and `Item_outlet_Sales`. This is also evident from the correlation number which we got above i.e. there is no strong correlation between these two variables\n",
    "- The second scatter plot between `Item_Visibility` and `Item_outlet_Sales`, there is no good relationship between them. But we can see a pattern that, as the `Item_Visibility` increases from **0.19**, the sales decreases. This might be due the reason that the management has given more visibility to those items which are not generally sold often, thinking that better visibility would increase the sales. This information can also help us to engineer new feature like - a categorical variable with categories - `high visibility` and `low visibility`. But we are not doing this here. This is an idea, we may want to pursue in future.\n",
    "- The third scatter plot between the variables - `Item_MRP` and `Item_outlet_Sales`, it is clear that there is a positive correlation between them and the variable `Item_MRP` would have a good predictive power to predict the sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wOo--wgcStwa"
   },
   "source": [
    "### Missing value treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rJH_X9GSStwb"
   },
   "source": [
    "Here we are imputing missing values for the variable `Item_Weight`. There are many ways to impute missing values, we can impute the missing values by its `mean`, `median` and using advanced imputation algorithm like `knn` etc. But here we are trying to find out some relationship of the variable `Item_Weight` with other variables in the dataset to impute those missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eqdaY1uKStwd"
   },
   "source": [
    "And also after imputing the missing values for the variables, the overall distribution of the variable should not change significantly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Upuae8zhStwf",
    "outputId": "a48bd8e9-73c2-4695-bcf8-0e657278b053"
   },
   "outputs": [],
   "source": [
    "fig= plt.figure(figsize=(18, 3))\n",
    "sns.heatmap(train_df.pivot_table(index='Item_Fat_Content', columns='Item_Type', values='Item_Weight'), annot=True);\n",
    "plt.xticks(rotation=45);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njfSIbGyStwi"
   },
   "source": [
    "#### Observations:\n",
    "\n",
    "- In the above heatmap, we can see that based on different combination of `Item_Types` and `Item_Fat_Content`, the average range of values for the column `Item_Weight` lies between the minimum value 10 and maximum value of 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kH_582RGStwj",
    "outputId": "61f395ed-d0c3-4c60-af80-7261cf8dfa0a"
   },
   "outputs": [],
   "source": [
    "fig= plt.figure(figsize=(18, 3))\n",
    "sns.heatmap(train_df.pivot_table(index='Outlet_Type', columns='Outlet_Location_Type', values='Item_Weight'), annot=True);\n",
    "plt.xticks(rotation=45);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q1XK0FukStwm"
   },
   "source": [
    "#### Observations:\n",
    "\n",
    "- In the above heatmap, we can see that based on different combination of `Outlet_Type` and `Outlet_Location_Type`, the average range of values for the column `Item_Weight` is constant at 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ITnRFJVGStwn",
    "outputId": "8e722b96-6db3-45f3-b575-5e09b3406ca1"
   },
   "outputs": [],
   "source": [
    "fig= plt.figure(figsize=(18, 3))\n",
    "sns.heatmap(train_df.pivot_table(index='Item_Fat_Content', columns='Outlet_Size', values='Item_Weight'), annot=True);\n",
    "plt.xticks(rotation=45);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HwTTVe_QStwp"
   },
   "source": [
    "#### Observations:\n",
    "\n",
    "- In the above heatmap, we can see that based on different combination of `Item_Fat_Content` and `Outlet_Size`, the average range of values for the column `Item_Weight` is also constant at 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vs6YidMQStwq"
   },
   "source": [
    "We will impute the missing values using an uniform distribution with parameters a=10 and b=14, as shown below - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fCrmr8gPStwr"
   },
   "outputs": [],
   "source": [
    "item_weight_indices_to_be_updated = train_df[train_df['Item_Weight'].isnull()].index\n",
    "train_df.loc[item_weight_indices_to_be_updated, 'Item_Weight'] = np.random.uniform(10, 14, \n",
    "                                                                                   len(item_weight_indices_to_be_updated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54JOZwC9Stws"
   },
   "source": [
    "Performing the same transformation on `test` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F8d612MgStwt"
   },
   "outputs": [],
   "source": [
    "item_weight_indices_to_be_updated = test_df[test_df['Item_Weight'].isnull()].index\n",
    "test_df.loc[item_weight_indices_to_be_updated, 'Item_Weight'] = np.random.uniform(10, 14, \n",
    "                                                                                   len(item_weight_indices_to_be_updated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1KMsKl4-Stwu"
   },
   "source": [
    "Next we will be imputing missing values for the column `Outlet_Size`. Below are creating two different datasets - one where we have non-null values for the column `Outlet_Size` and in the other dataset we all the values of the column `Outlet_Size` are missing. We then check the distribution of the other variables in the dataset where `Outlet_Size` is missing to identify if there is any pattern present or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5akZ0BuQStww"
   },
   "outputs": [],
   "source": [
    "outlet_size_data = train_df[train_df['Outlet_Size'].notnull()]\n",
    "outlet_size_missing_data = train_df[train_df['Outlet_Size'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xSQCXKOBStwx",
    "outputId": "d1cba4b3-28ef-452c-a224-886136a97c9a"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "  \n",
    "fig.suptitle('Bar plot for all categorical variables in the dataset where the variable Outlet_Size is missing')\n",
    "  \n",
    "\n",
    "sns.countplot(ax=axes[0], x='Outlet_Type', data=outlet_size_missing_data, color='blue', \n",
    "              order=outlet_size_missing_data['Outlet_Type'].value_counts().index, );\n",
    "\n",
    "sns.countplot(ax=axes[1], x='Outlet_Location_Type', data=outlet_size_missing_data, color='blue', \n",
    "              order=outlet_size_missing_data['Outlet_Location_Type'].value_counts().index);\n",
    "\n",
    "sns.countplot(ax=axes[2], x='Item_Fat_Content', data=outlet_size_missing_data, color='blue', \n",
    "              order=outlet_size_missing_data['Item_Fat_Content'].value_counts().index);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "min9GNmMStwz"
   },
   "source": [
    "#### Observations:\n",
    "\n",
    "- We can see that, in the dataset, wherever `Outlet_Size` is missing majority of them have `Outlet_Type` as Supermarket Type 1 and `Outlet_Location_Type` as Tier 2 and `Item_Fat_Content` as `Low Fat`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dzZ5_iPtStw1"
   },
   "source": [
    "Now are creating a crosstab of all the above categorical variables against the column `Outlet_Size` where we want to impute the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x0a9piRPStw2",
    "outputId": "6d148b81-a7a6-4af9-aa57-69866c445025"
   },
   "outputs": [],
   "source": [
    "fig= plt.figure(figsize=(18, 3))\n",
    "sns.heatmap(pd.crosstab(index=outlet_size_data['Outlet_Type'], columns=outlet_size_data['Outlet_Size']), annot=True, fmt='g')\n",
    "plt.xticks(rotation=45);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MreHi_luStw3"
   },
   "source": [
    "#### Observations: \n",
    "\n",
    "- We can see from the crosstab, all the grocery stores have `Outlet_Size` as small\n",
    "- And all the Supermarket Type 2 and Supermarket Type 3 have `Outlet_Size` as Medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DpKW7JI9Stw5",
    "outputId": "2e591889-842e-484a-e709-3cde7affd55d"
   },
   "outputs": [],
   "source": [
    "fig= plt.figure(figsize=(18, 3))\n",
    "sns.heatmap(pd.crosstab(index=outlet_size_data['Outlet_Location_Type'], columns=outlet_size_data['Outlet_Size']), annot=True, \n",
    "            fmt='g')\n",
    "plt.xticks(rotation=45);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VUXew-fIStw7"
   },
   "source": [
    "#### Observations:\n",
    "\n",
    "- We can see from the crosstab, all the Tier 2 stores have `Outlet_Size` as Small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yJbyRZ0EStw9",
    "outputId": "86ec9d8c-636d-4de0-98c9-b2f33ebbe97e"
   },
   "outputs": [],
   "source": [
    "fig= plt.figure(figsize=(18, 3))\n",
    "sns.heatmap(pd.crosstab(index=outlet_size_data['Item_Fat_Content'], columns=outlet_size_data['Outlet_Size']), annot=True, \n",
    "            fmt='g')\n",
    "plt.xticks(rotation=45);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S6rBxSKqStw_"
   },
   "source": [
    "#### Observations:\n",
    "\n",
    "- There is no clear pattern between the variables `Item_Fat_Content` and `Outlet_Size` unlike what we have got against `Outlet_Type` and `Outlet_Location_Type`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WEaP9j5XStxA"
   },
   "source": [
    "Now we will use the patterns we have from the variables `Outlet_Type` and `Outlet_Location_Type` to impute the missing values for the column `Outlet_Size`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qEuDlf60StxB"
   },
   "source": [
    "Below we are identifying the indices in the dataframe where `Outlet_Size` is null/missing and `Outlet_Type` is Grocery Store, so that we can replace those missing values with the value Small based on the pattern we have identified in above crosstab visualization. Similarly we are also identifying the indices in the dataframe where `Outlet_Size` is null/missing and `Outlet_Location_Type` is Tier 2, to impute those missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ajJr8yY_StxD"
   },
   "outputs": [],
   "source": [
    "grocery_store_indices = train_df[train_df['Outlet_Size'].isnull()].query(\"Outlet_Type == 'Grocery Store'\").index\n",
    "tier_2_indices = train_df[train_df['Outlet_Size'].isnull()].query(\"Outlet_Location_Type == 'Tier 2'\").index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1JTzudChStxE"
   },
   "source": [
    "Now are updating those indices for the column `Outlet_Size` with the value Small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9KuCG1UmStxH"
   },
   "outputs": [],
   "source": [
    "train_df.loc[grocery_store_indices, 'Outlet_Size'] = 'Small'\n",
    "train_df.loc[tier_2_indices, 'Outlet_Size'] = 'Small'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Pp4PRgdStxI"
   },
   "source": [
    "Performing the same transformation on `test` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nm3O5DgiStxJ"
   },
   "outputs": [],
   "source": [
    "grocery_store_indices = test_df[test_df['Outlet_Size'].isnull()].query(\"Outlet_Type == 'Grocery Store'\").index\n",
    "tier_2_indices = test_df[test_df['Outlet_Size'].isnull()].query(\"Outlet_Location_Type == 'Tier 2'\").index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aEoDE-9nStxK"
   },
   "outputs": [],
   "source": [
    "test_df.loc[grocery_store_indices, 'Outlet_Size'] = 'Small'\n",
    "test_df.loc[tier_2_indices, 'Outlet_Size'] = 'Small'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l-_aypfmStxL"
   },
   "source": [
    "After we have imputed the missing values, let's check again if we still have missing values in both train and test datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xgNlrYp1StxM",
    "outputId": "24af7a78-9c1f-4bfc-98f7-38013f5cc6df"
   },
   "outputs": [],
   "source": [
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bkspTN6-StxN",
    "outputId": "d822b2cb-0421-4bab-8547-22bb21308ccc"
   },
   "outputs": [],
   "source": [
    "test_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7Jj7hcbStxP"
   },
   "source": [
    "As we can see that there is no more missing values in the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XU2sr5oDStxQ"
   },
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "115VfFYTStxS"
   },
   "source": [
    "Now that, we have completed the data understanding and data preparation step, before starting the modeling task, we think that there are certain features which are not present in the dataset, but we can create using the existing columns, which we think can have the predictive power to predict the sales. This step of creating a new feature from the existing features in the dataset is known as **Feature Engineering**. So we will start with a hypothesis - As the store gets older, the sales increases. Now how do we define old? We know the establishment year and this data is collected in 2013, so the age can be found by subtracting establishment year from 2013. This is what we are doing in the below piece of code-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Cg0wgDbStxU"
   },
   "source": [
    "We are creating the a new feature `Outlet_Age` which indicates that how old the outlet is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lTd3lDu_StxV"
   },
   "outputs": [],
   "source": [
    "train_df['Outlet_Age'] = 2013 - train_df['Outlet_Establishment_Year']\n",
    "test_df['Outlet_Age'] = 2013 - test_df['Outlet_Establishment_Year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bfexkfw-StxX",
    "outputId": "abc7e122-0884-4f56-ea4f-687bcadbdb19"
   },
   "outputs": [],
   "source": [
    "fig= plt.figure(figsize=(18, 6))\n",
    "sns.boxplot(x='Outlet_Age', y='Item_Outlet_Sales', data=train_df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WlDXEF2IStxa"
   },
   "source": [
    "**Observations:**\n",
    "* The hypothesis that we had - **As the store gets older, the sales increases** does not seem to hold true based on the above plot. Because for different age of stores the sales have similar distribution approximately. But let's keep this variable as of now, and we will revisit this variable again at the time of model building and we remove this variable by observing its significance later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bxq--d5CStxc"
   },
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wmWQcrmaStxd"
   },
   "source": [
    "Now, we have analyzed the all the variables in the dataset, we are now ready to start building the model. We have observed that not all the independent variables are important to predict the outcome variable. But at the beginning, we will use all the variables and then from the model summary we will take decision which variable to remove from the model. Model building is an iterative task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RT-S3iO9Stxf"
   },
   "outputs": [],
   "source": [
    "# we are removing the outcome variable from the feature set and also the variable Outlet_Establishment_Year as we have created\n",
    "# a new variable Outlet_Age\n",
    "train_features = train_df.drop(['Item_Outlet_Sales', 'Outlet_Establishment_Year'], axis=1)\n",
    "\n",
    "# and then we are extracting the outcome variable separately\n",
    "train_target = train_df['Item_Outlet_Sales']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "POVz5eD5Stxh"
   },
   "source": [
    "In linear based models, whenever we have categorical variables as independent variables, we need to create **one hot encoded** representation (which is also known as dummy variables) of those categorical variables. The below piece of code is creating dummy variables and we are removing the first category in those variables which is known as **reference variable**. The reference variable helps to interpret the linear regression which we will see later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UgYmr_a-Stxi",
    "outputId": "29fd4a15-b1b2-4aff-f8db-32f2be35754b"
   },
   "outputs": [],
   "source": [
    "# in linear based models it is mandatory to create dummy variables for the categorical variables\n",
    "train_features = pd.get_dummies(train_features, drop_first=True)\n",
    "train_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDYJ4S4YStxj"
   },
   "source": [
    "#### Observations:\n",
    "\n",
    "- Notice the column names of all the categorical variables, and also the overall number of columns has increased after we created the dummy variables\n",
    "- And for each of those categorical variables, the first category have been removed, e.g. - the category `Low Fat` of the categorical variable `Item_Fat_Content` have been removed which became the `reference variable` and we only have the category `Regular` as a new column `Item_Fat_Content_Regular`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vf12dooCStxk"
   },
   "source": [
    "Below we are scaling the numerical variables in the dataset to have same range. If we don't do this, then the model will be biased towards a variable where we have a higher range and the model will not learn from the variables which values with lower range. There are many ways to do scaling, here we are using `MinMaxScaler` as we have both categorical and numerical variables in the dataset and we don't want to change the dummy encodings of the categorical variables that we have already created. For more information on different ways of doing scaling, refer to the **section 6.3.1** of this page [here](https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4JEtCIPMStxl",
    "outputId": "5f075df2-38b7-4137-e32c-1f25da2f5c26"
   },
   "outputs": [],
   "source": [
    "# creating an instance of the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# applying fit_transform on the training features data\n",
    "train_features_scaled = scaler.fit_transform(train_features)\n",
    "\n",
    "# the above scaler returns the data in array format, below we are converting back to pandas dataframe\n",
    "train_features_scaled = pd.DataFrame(train_features_scaled, index=train_features.index, columns=train_features.columns)\n",
    "train_features_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76SIsm9FStxm"
   },
   "source": [
    "Now as the dataset is ready and prepared, we are set to build the model using `statsmodels` package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "srYbzDrLStxn"
   },
   "outputs": [],
   "source": [
    "# here we are adding the intercept term\n",
    "train_features_scaled = sm.add_constant(train_features_scaled)\n",
    "\n",
    "# calling the OLS algorithm on the train features and target variable\n",
    "ols_model_0 = sm.OLS(train_target, train_features_scaled)\n",
    "\n",
    "# fitting the model\n",
    "ols_res_0 = ols_model_0.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Um2UmCE7Stxo"
   },
   "source": [
    "Now let's observe summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GLdEo2EwStxo",
    "outputId": "d931b5fb-ce0a-46bf-9d22-5ab045f6e9ac"
   },
   "outputs": [],
   "source": [
    "print(ols_res_0.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s2VREEPjStxp"
   },
   "source": [
    "- We can see that `R-squared` for the model is `0.563`. \n",
    "- Not all the variables are statistically significant to predict the outcome variable. To check which are statistically significant or have predictive power to predict the target variable, we need to check the `p-value` against all the independent variables.\n",
    "\n",
    "**Interpreting the Regression Results:**\n",
    "\n",
    "1. **Adjusted. R-squared**: It reflects the fit of the model.\n",
    "    - R-squared values range from 0 to 1, where a higher value generally indicates a better fit, assuming certain conditions are met.\n",
    "    - In our case, the value for Adj. R-squared is **0.562**\n",
    "\n",
    "2. **coeff**: It represents the change in the output Y due to a change of one unit in the variable (everything else held constant).\n",
    "3. **std err**: It reflects the level of accuracy of the coefficients.\n",
    "    - The lower it is, the more accurate the coefficients are.\n",
    "4. **P >|t|**: It is p-value.\n",
    "   \n",
    "   * Pr(>|t|) : For each independent feature there is a null hypothesis and alternate hypothesis \n",
    "\n",
    "    Ho : Independent feature is not significant \n",
    "   \n",
    "    Ha : Independent feature is significant \n",
    "    \n",
    "   * A p-value of less than 0.05 is considered to be statistically significant.\n",
    "\n",
    "   \n",
    "5. **Confidence Interval**: It represents the range in which our coefficients are likely to fall (with a likelihood of 95%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZLvLzGeStxq"
   },
   "source": [
    "To understand in detail, how p-values can help to identify statistically significant variables to predict the sales, we need to understand the hypothesis testing framework here - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cm7rO6TsStxr"
   },
   "source": [
    "In the following example, we are showing the null hypothesis between the independent variable `Outlet_Size` and the dependent variable `Item_Outlet_Sales` to identify if there is any relationship between them or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vnksNRw1Stxs"
   },
   "source": [
    "- **Null hypothesis:** There is nothing going on or there is no relationship between variables `Outlet_Size` and `Item_Outlet_Sales` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZuS0JEhrStxt"
   },
   "source": [
    "<center>$\\mu_{\\text {outlet size }=\\text { High }}=\\mu_{\\text {outlet size }=\\text { Medium }}=\\mu_{\\text {outlet size }=\\text { Small }}$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BE2MSOv3Stxt"
   },
   "source": [
    "- **Alternate hypothesis:** There is something going on, because of which there is a relationship between variables `Outlet_Size` and `Item_Outlet_Sales` i.e. - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xXCeljJTStxv"
   },
   "source": [
    "<center>$\\mu_{\\text {outlet size = High }} \\neq \\mu_{\\text {outlet size = Medium }} \\neq \\mu_{\\text {outlet size }=\\text { small }}$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uV2OFTpOStxw"
   },
   "source": [
    "and from the above model summary if p-value less than the significance level 0.05, that means we will reject the null hypothesis in favor of alternate hypothesis. In other words, we have enough statistical evidence that there is relationship exists between the variables `Outlet_Size` and `Item_Outlet_Sales`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WU8lYvTcStxw"
   },
   "source": [
    "Based on this above analysis, if we observe the above model summary, we can see only some variables or some categories of a categorical variables have p-value lower than 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_VqM9JOStxx"
   },
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yx_FCiTvStxy"
   },
   "source": [
    "As we have discussed above, that not all variables are statistically significant to predict the dependent/outcome variable. In this step, we are selecting only the relevant features `based on p-value` which we discussed above and eliminating all the other variables that are not important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OZPg3CU_Stxy"
   },
   "outputs": [],
   "source": [
    "# selecting only relevant features\n",
    "train_features_2 = train_df[['Item_MRP', 'Outlet_Age', 'Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zAYacAtyStxy"
   },
   "source": [
    "Again we are creating the dummy variables for the categorical variables as we discussed above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h4Ov8V6IStxz",
    "outputId": "67f74ad6-31df-4009-c088-73879bc82d0c"
   },
   "outputs": [],
   "source": [
    "train_features_2 = pd.get_dummies(train_features_2, drop_first=True)\n",
    "train_features_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mjkG6ugyStx0"
   },
   "source": [
    "Then we are scaling the variables to have the same scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BPM1tH7qStx0",
    "outputId": "435399ba-ee60-42ca-d537-5779027b76b5"
   },
   "outputs": [],
   "source": [
    "train_features_scaled_2 = scaler.fit_transform(train_features_2)\n",
    "train_features_scaled_2 = pd.DataFrame(train_features_scaled_2, index=train_features_2.index, columns=train_features_2.columns)\n",
    "train_features_scaled_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QnLV8EskStx1"
   },
   "source": [
    "And now build the model again, but this time with only taking the relevant variables based on p-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NN0oD4waStx1"
   },
   "outputs": [],
   "source": [
    "train_features_scaled_2 = sm.add_constant(train_features_scaled_2)\n",
    "\n",
    "ols_model_2 = sm.OLS(train_target, train_features_scaled_2)\n",
    "ols_res_2 = ols_model_2.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nxb-ZzKzStx2",
    "outputId": "8adfd7d8-e066-41d7-90b7-35d31ce54794"
   },
   "outputs": [],
   "source": [
    "print(ols_res_2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-6qMLLqqStx3"
   },
   "source": [
    "**Observations**\n",
    "- The R-Squared value did not change at all. It is still coming out to be 0.563 which implies that all other variables were not adding any value to the model.\n",
    "\n",
    "Lets' check assumptions of the linear regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xHzakgF2Stx3"
   },
   "source": [
    "### Checking for assumptions and rebuilding the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bJ02Do_mStx4"
   },
   "source": [
    "In this step, we will check for the below assumptions in the model, to check if they hold true or not. And if there is any issue, then we will rebuild the model after fixing those issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_eyMW7MStx4"
   },
   "source": [
    "1. No multicollinearity among independent variables\n",
    "2. Mean of residuals should be 0\n",
    "3. Normality of error terms\n",
    "4. Linearity of variables\n",
    "5.  No heteroscedasticity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V2dUJGd4Stx5"
   },
   "source": [
    "#### No multicollinearity among independent variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PW9vRPaIStx5"
   },
   "source": [
    "* Multicollinearity occurs when predictor variables in a regression model are correlated. This correlation is a problem because predictor variables should be independent.  If the correlation between variables is high, it can cause problems when we fit the model and interpret the results. When we have multicollinearity in the linear model, the coefficients that the model suggests are unreliable.\n",
    "\n",
    "* There are different ways of detecting (or testing) multi-collinearity, one such way is Variation Inflation Factor.\n",
    "\n",
    "* **Variance  Inflation  factor**:  Variance  inflation  factors  measures  the  inflation  in  the variances of the regression parameter estimates due to collinearities that exist among the  predictors.  It  is  a  measure  of  how  much  the  variance  of  the  estimated  regression coefficient k is inflated by  the  existence  of  correlation  among  the  predictor variables in the model. \n",
    "\n",
    "* General Rule of thumb: If VIF is 1 then there is no correlation among the kth predictor and the remaining predictor variables, and  hence  the variance of k is not inflated at all. Whereas if VIF exceeds 5 or is close to exceeding 5, we say there is moderate VIF and if it is 10 or exceeding 10, it shows signs of high multi-collinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Txb9YsZ2Stx7",
    "outputId": "ee38b0c5-e15c-4002-f92f-499348821c86"
   },
   "outputs": [],
   "source": [
    "# we drop the one with the highest vif value and check the Adjusted-R Squared\n",
    "vif_series_1 = pd.Series([variance_inflation_factor(train_features_scaled_2.values,i) for i in range(train_features_scaled_2.shape[1])],\n",
    "                        index=train_features_scaled_2.columns)\n",
    "print('Series before feature selection: \\n\\n{}\\n'.format(vif_series_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aRnZoO5wStx7"
   },
   "source": [
    "#### Removing Multicollinearity\n",
    " * To remove multicollinearity, we will drop the column that has VIF score greater than 5 and build the model again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YBYspTkhStx8"
   },
   "source": [
    "We can see that the variable `Outlet_Size_Medium` has the highest VIF, so in the next iteration we will remove this variable and build the model again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dSxneoWJStx9"
   },
   "outputs": [],
   "source": [
    "train_features_scaled_3 = train_features_scaled_2.drop(['Outlet_Size_Medium'], axis=1)\n",
    "ols_model_3 = sm.OLS(train_target, train_features_scaled_3)\n",
    "ols_res_3 = ols_model_3.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MUwSqVA9Stx9",
    "outputId": "e10a3286-c1f3-486f-ec3e-1dfaad49ef9e"
   },
   "outputs": [],
   "source": [
    "print(ols_res_3.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sFNLhVekStx-"
   },
   "source": [
    "Now, let's check the VIF again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wqSKj1tqStx-",
    "outputId": "ae14fbd4-bf74-4e46-d964-d79dd8600bb7"
   },
   "outputs": [],
   "source": [
    "vif_series_2 = pd.Series([variance_inflation_factor(train_features_scaled_3.values,i) for i in range(train_features_scaled_3.shape[1])],\n",
    "                        index=train_features_scaled_3.columns)\n",
    "print('Series before feature selection: \\n\\n{}\\n'.format(vif_series_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6lOjfst1Stx_"
   },
   "source": [
    "- VIF score for `Outlet_Type_Supermarket Type2` is the highest and more than 5. Let's remove this variable and build the model again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EPoIOgQCStyA"
   },
   "outputs": [],
   "source": [
    "train_features_scaled_4 = train_features_scaled_3.drop(['Outlet_Type_Supermarket Type2'], axis=1)\n",
    "ols_model_4 = sm.OLS(train_target, train_features_scaled_4)\n",
    "ols_res_4 = ols_model_4.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aOex071GStyA",
    "outputId": "a14cfd9f-01e1-4e45-b8ad-eee63275fe47"
   },
   "outputs": [],
   "source": [
    "print(ols_res_4.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vgnzs8tSStyB",
    "outputId": "5ea6921c-1077-4ba2-b501-1ba1927f3444"
   },
   "outputs": [],
   "source": [
    "vif_series_3 = pd.Series([variance_inflation_factor(train_features_scaled_4.values,i) for i in range(train_features_scaled_4.shape[1])],\n",
    "                        index=train_features_scaled_4.columns)\n",
    "print('Series before feature selection: \\n\\n{}\\n'.format(vif_series_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Wrj4hkfStyB"
   },
   "source": [
    "**Observations**\n",
    "- We have removed the multi-collinearity from the data as all the variables have VIF score less than 5 (except constant)\n",
    "- But some of the variables in the model have become insignificant. Let's drop these variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c_rNnHU6StyC"
   },
   "outputs": [],
   "source": [
    "train_features_scaled_5 = train_features_scaled_4.drop(['Outlet_Location_Type_Tier 2', 'Outlet_Location_Type_Tier 3'], axis=1)\n",
    "ols_model_5 = sm.OLS(train_target, train_features_scaled_5)\n",
    "ols_res_5 = ols_model_5.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xZvs_GM5StyC",
    "outputId": "d9ae126d-059c-4b0f-b467-fc5a7a14f0ae"
   },
   "outputs": [],
   "source": [
    "print(ols_res_5.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-cxLqW-bStyC"
   },
   "source": [
    "**Observations**\n",
    "- All the variables are significant now and the R-Squared has not decreased by much. It is still coming out to be 0.55.\n",
    "\n",
    "Let's check other assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IDvXb8s3StyD"
   },
   "source": [
    "### Mean of residuals should be 0 and normality of error terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2wQVSHPNStyD"
   },
   "outputs": [],
   "source": [
    "residual = ols_res_5.resid # Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ioxkrPdStyD",
    "outputId": "0d76b822-226d-40fb-eadb-a19f8aeb9486"
   },
   "outputs": [],
   "source": [
    "residual.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aWokAlsJStyE"
   },
   "source": [
    "The mean of residuals is very close to 0. Hence, the corresponding assumption is satisfied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aI4_xR2yStyE"
   },
   "source": [
    "### Test for Normality\n",
    "\n",
    "**What is the test?**\n",
    "\n",
    "* Error terms/Residuals should be normally distributed\n",
    "\n",
    "* If the error terms are non- normally distributed, confidence intervals may become too wide or narrow. Once confidence interval becomes unstable, it leads to difficulty in estimating coefficients based on minimization of least squares.\n",
    "\n",
    "**What do non-normality indicate?**\n",
    "\n",
    "* It suggests that there are a few unusual data points which must be studied closely to make a better model.\n",
    "\n",
    "**How to Check the Normality?**\n",
    "\n",
    "* It can be checked via QQ Plot. Residuals following normal distribution will make a straight line plot otherwise not.\n",
    "\n",
    "* Other test to check for normality : Shapiro-Wilk test.\n",
    "\n",
    "**What is the residuals are not-normal?**\n",
    "\n",
    "* We can apply transformations like log, exponential, arcsinh, etc as per our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ovgeB61FStyE",
    "outputId": "a8c72d20-cc03-43d5-a7fa-c75f9e8cc2c0"
   },
   "outputs": [],
   "source": [
    "# Plot histogram of residuals\n",
    "sns.histplot(residual, kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3Ir2ZpaStyF"
   },
   "source": [
    "We can see that the error terms are normally distributed. The assumption of normality is satisfied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBRU_BQDStyF"
   },
   "source": [
    "### Linearity of variables\n",
    "\n",
    "It states that the predictor variables must have a linear relation with the dependent variable.\n",
    "\n",
    "To test the assumption, we'll plot residuals and fitted values on a plot and ensure that residuals do not form a strong pattern. They should be randomly and uniformly scattered on the x-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hfDsNikpStyF",
    "outputId": "a00c5499-ae14-4258-a76f-0d96d8c81dd0"
   },
   "outputs": [],
   "source": [
    "# predicted values\n",
    "fitted = ols_res_5.fittedvalues\n",
    "\n",
    "# sns.set_style(\"whitegrid\")\n",
    "sns.residplot(x = fitted, y = residual, color=\"lightblue\")\n",
    "plt.xlabel(\"Fitted Values\")\n",
    "plt.ylabel(\"Residual\")\n",
    "plt.title(\"Residual PLOT\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VNG5Yw8WStyG"
   },
   "source": [
    "**Observations**\n",
    "- We can see that there is some pattern in fitted values and residuals i.e. the residuals are not randomly distributed.\n",
    "- Let's try to fix this. We can apply the log transformation on the target variable and try to build a new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ecG-EHp1StyG"
   },
   "outputs": [],
   "source": [
    "#Log transformation on the target variable\n",
    "train_target_log = np.log(train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SsqACrL5StyH"
   },
   "outputs": [],
   "source": [
    "#Fitting new model with the transformed target variable\n",
    "ols_model_6 = sm.OLS(train_target_log, train_features_scaled_5)\n",
    "ols_res_6 = ols_model_6.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D66nKbjhStyH",
    "outputId": "288b6662-700e-4016-8c65-85c6a2d9c9a4"
   },
   "outputs": [],
   "source": [
    "# predicted values\n",
    "fitted = ols_res_6.fittedvalues\n",
    "residual1 = ols_res_6.resid\n",
    "\n",
    "# sns.set_style(\"whitegrid\")\n",
    "sns.residplot(x = fitted, y = residual1, color=\"lightblue\")\n",
    "plt.xlabel(\"Fitted Values\")\n",
    "plt.ylabel(\"Residual\")\n",
    "plt.title(\"Residual PLOT\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXE6BbSTStyI"
   },
   "source": [
    "**Observations**\n",
    "- We can see that there is no pattern in the residuals vs fitted values scatter plot now i.e. the linearity assumption is satisfied.\n",
    "- Let's check the model summary of the latest model we have fit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nk1qfEoAStyI",
    "outputId": "e5e20d8f-474e-4cb8-a574-15e3e9b4413a"
   },
   "outputs": [],
   "source": [
    "print(ols_res_6.summary(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7CNX8nYpStyI"
   },
   "source": [
    "- The model performance has improved significantly. The R-Squared has increased from 0.55 to 0.675.\n",
    "\n",
    "Let's check the final assumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kfmdhTOmStyJ"
   },
   "source": [
    "### No heteroscedasticity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g2c-OV0HStyJ"
   },
   "source": [
    "#### Test for homoscedasticity\n",
    "\n",
    "* homoscedasticity - If the variance of the residuals are symmetrically distributed across the regression line, then the data is said to homoscedastic.\n",
    "\n",
    "* heteroscedasticity - If the variance is unequal for the residuals across the regression line, then the data is said to be heteroscedastic. In this case the residuals can form an arrow shape or any other non symmetrical shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EtcxB77MStyJ"
   },
   "source": [
    "We will use GoldfeldQuandt test to check homoscedasticity.\n",
    "\n",
    "Null hypothesis : Residuals are homoscedastic\n",
    "\n",
    "Alternate hypothesis : Residuals are hetroscedastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3TucwhK7StyK"
   },
   "outputs": [],
   "source": [
    "from statsmodels.stats.diagnostic import het_white\n",
    "from statsmodels.compat import lzip\n",
    "import statsmodels.stats.api as sms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LP1NYmW7StyK",
    "outputId": "dc3521fd-61dc-4f48-a678-c36598d09c85"
   },
   "outputs": [],
   "source": [
    "import statsmodels.stats.api as sms\n",
    "from statsmodels.compat import lzip\n",
    "\n",
    "name = [\"F statistic\", \"p-value\"]\n",
    "test = sms.het_goldfeldquandt(train_target_log, train_features_scaled_5)\n",
    "lzip(name, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j1zQOvFJStyL"
   },
   "source": [
    "- As we can see from the above test the p-value is greater than 0.05, so we fail to reject the null-hypothesis. That means - residuals are homoscedastic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKhAWyx1StyL"
   },
   "source": [
    "We have verified all the assumptions of the linear regression model. The final equation of the model is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YqOh4hJyStyL"
   },
   "source": [
    "**$\\log ($ Item_Outlet_Sales $)$ $=6.1604 + 1.9623 *$ Item_MRP - $1.0049 *$ Outlet_Age $ - 0.5812 *$ Outlet_Size_Small $+1.2249 *$ Outlet_Type_Supermarket Type1 $ + 1.9662 *$ Outlet_Type_Supermarket Type3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0MAtF2Q2StyM"
   },
   "source": [
    "Now we finalized with the model, let's prepare the test dataset to predict the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zvnyXRqlStyM",
    "outputId": "d2192a5b-c2c4-4e36-a19f-6795216b792f"
   },
   "outputs": [],
   "source": [
    "without_const=train_features_scaled_2.iloc[:,1:]\n",
    "test_features = pd.get_dummies(test_df, drop_first=True)\n",
    "test_features=test_features[list(without_const)]\n",
    "test_features_scaled = scaler.transform(test_features)\n",
    "test_features_scaled = pd.DataFrame(test_features_scaled, columns=without_const.columns)\n",
    "test_features_scaled = sm.add_constant(test_features_scaled)\n",
    "test_features_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fi-Zey0lStyM"
   },
   "outputs": [],
   "source": [
    "test_features_scaled = test_features_scaled[train_features_scaled_5.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cGM8GY2cStyN"
   },
   "source": [
    "### Evaluation Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uCq3YWMvStyN"
   },
   "source": [
    "#### R-squared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zpEQfD9mStyN"
   },
   "source": [
    "The R-squared metric gives us an indication that how good/bad our model is from a baseline model. Here, we have explained ~98% variance in the data as compared to the baseline model when there is no independent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O3QpcKLaStyN",
    "outputId": "20d110ec-0386-42f7-943e-6a16520c07b5"
   },
   "outputs": [],
   "source": [
    "print(ols_res_6.rsquared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0NOqkCE4StyO"
   },
   "source": [
    "#### Mean Squared Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-jWvUtuStyO"
   },
   "source": [
    "This metric measures the average of the squares of the errors i.e. the average squared difference between the estimated values and the actual value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GgNVfmUEStyO",
    "outputId": "1a2fffa9-cb7b-47ff-8d70-4028ce13199d"
   },
   "outputs": [],
   "source": [
    "print(ols_res_6.mse_resid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ND_oJoYStyP"
   },
   "source": [
    "#### Root Mean Squared Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aj_g2UwkStyP"
   },
   "source": [
    "This metric is same as the above but instead of square root of MSE to get RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kDlVQ87AStyP",
    "outputId": "75785e77-a5f1-4d75-debb-0116b044baa3"
   },
   "outputs": [],
   "source": [
    "print(np.sqrt(ols_res_6.mse_resid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-Tf7VbeStyQ"
   },
   "source": [
    "Below we are checking the cross validation score to identify if the model that we have built is `underfitted`, `overfitted` or `just right` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LokqPuwUStyQ",
    "outputId": "2fd48696-a79a-4da4-8628-f4b596c2e267"
   },
   "outputs": [],
   "source": [
    "#Fitting linear model\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "linearregression = LinearRegression()                                    \n",
    "\n",
    "cv_Score11 = cross_val_score(linearregression, train_features_scaled_5, train_target_log, cv = 10)\n",
    "cv_Score12 = cross_val_score(linearregression, train_features_scaled_5, train_target_log, cv = 10, \n",
    "                             scoring = 'neg_mean_squared_error')                                  \n",
    "\n",
    "\n",
    "print(\"RSquared: %0.3f (+/- %0.3f)\" % (cv_Score11.mean(), cv_Score11.std() * 2))\n",
    "print(\"Mean Squared Error: %0.3f (+/- %0.3f)\" % (-1*cv_Score12.mean(), cv_Score12.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GU0OOAdeStyR"
   },
   "source": [
    "**Observations:**\n",
    "- The R-Squared on the cross validation is 0.673 which is almost similar to the R-Squared on the training dataset.\n",
    "- And the MSE on cross validation is 0.337 which is almost similar to the R-Squared on the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d_bjff0XStyR"
   },
   "source": [
    "It seems like that our model is `just right`. It is giving a generalized performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcoUwRbUStyR"
   },
   "source": [
    "Since this model that we have developed is a linear model, which is not capable of capturing non-linear patterns in the data, so we may want to build more advanced regression model which can capture the non-linearities in the data and improve this model further. But that is out of scope for this case study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKXCINi3StyR"
   },
   "source": [
    "### Predicting on the test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QRhOPgSgStyS"
   },
   "source": [
    "Once our model is completed, we can now use this to predict the sales in our test data as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gcahY82jStyS"
   },
   "outputs": [],
   "source": [
    "# this test predictions will be on a log scale\n",
    "test_predictions = ols_res_6.predict(test_features_scaled)\n",
    "\n",
    "# we are converting the log scale predictions to its original scale\n",
    "test_predictions_inverse_transformed = np.exp(test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1k5kO5HaStyS"
   },
   "source": [
    "Point to remember: The output of this model is in log scale. So after prediction we need to transform this value in log back to its original scale by doing inverse of log transformation i.e. taking exponentiation</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xshtuy1XStyT",
    "outputId": "9a773d34-f112-4b3e-abc3-39a99f4e3760"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "sns.histplot(test_predictions, ax=ax[0]);\n",
    "sns.histplot(test_predictions_inverse_transformed, ax=ax[1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J0MB2PtMStyT"
   },
   "source": [
    "### Conclusion and Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gsA0Dq2eStyT"
   },
   "source": [
    "- We performed EDA - univariate and bivariate analysis on all the variables in the dataset\n",
    "- We then performed missing values treatment using relationship between variables\n",
    "- We started the model building the process with all the features\n",
    "- We analyzed the model summary report\n",
    "- Then we checked for different assumptions of linear regression\n",
    "- And we then fixed the model iteratively if any assumptions did not hold true\n",
    "- Then we evaluated the model using different evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lxDMyAliStyU"
   },
   "source": [
    "Lastly below is the model equation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DrmenJwgStyU"
   },
   "source": [
    "**$\\log ($ Item_Outlet_Sales $)$ $= 6.1604 + 1.9623 *$ Item_MRP - $1.0049 *$ Outlet_Age $ - 0.5812 *$ Outlet_Size_Small $+1.2249 *$ Outlet_Type_Supermarket Type1 $ + 1.9662 *$ Outlet_Type_Supermarket Type3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iZssJBBJStyU"
   },
   "source": [
    "- From this above equation, we can interpret that - with one unit change in the variable `Item_MRP` the outcome variable log of `Item_Outlet_Sales` increases by 1.9623 units. So if we want to increase sales we may want to store higher MRP items in the high visibility area.\n",
    "\n",
    "- On an average, the log sales of stores with outlet size small is 0.5812 less than the log sales of outlet size high (remember the **reference variable**, here for the variable outlet size - **the category high was the reference variable which we dropped at the time of creating the dummy variable**). So when we are interpreting the coefficients of categorical variables we need to compare them against its reference variable. So the recommendation to the management would be to focus more on the larger stores. May be they can focus on having more inventory in larger stores as compared to others as we don't want to loose on revenue because unavailability of items there.\n",
    "\n",
    "- On an average, the log sales of stores with type Supermarket type 1 is 1.2249 more than the log sales of stores with type grocery stores (the **reference variable** for this categorical variable **Outlet_Type**, here is **grocery store**). And similarly for Supermarket Type 3, on an average, have 1.9662 times more log sales than grocery stores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fI4LvBDeStyV"
   },
   "source": [
    "So after interpreting this linear regression equation it is clear that large stores of supermarket type 3 have more sales than others. So we want to maintain or improve the sales in these stores. And for the remaining ones we may want to make strategies to improve the sales e.g. providing better customer service, better training for store staffs, providing more visibility of high MRP item, etc."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "hxo88e_CStwZ"
   ],
   "name": "Practice Case Study_Linear Regression.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
